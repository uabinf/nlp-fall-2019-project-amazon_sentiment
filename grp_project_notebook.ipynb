{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Domain Sentiment Analysis Study\n",
    "NLP Group Project\n",
    "\n",
    "Atulya Shetty, Payton Walker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Normalize data\n",
    "def imdb_data(test_size=0.2):\n",
    "    \"\"\" \n",
    "        Retrieves IMDB data movie review dataset\n",
    "        \n",
    "        Parameters:\n",
    "        test_size = float, optional (default=0.2)\n",
    "        \n",
    "        Returns:\n",
    "        Tuple containing training and test data for IMBD movie            [po;iuyreviews and their sentiment\n",
    "        \n",
    "    \"\"\"\n",
    "    imdb_file = 'data/imdb.csv'\n",
    "    imdb_df = pd.read_csv(imdb_file)\n",
    "    X, y = imdb_df.review, imdb_df.sentiment\n",
    "    return train_test_split(X, y, test_size= 0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train_review, imdb_test_review, imdb_train_sent, imdb_test_sent = imdb_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Normalize data\n",
    "def yelp_data():\n",
    "    \n",
    "    \"\"\" \n",
    "        Parameters:\n",
    "        test_size = float, optional (default=0.2)\n",
    "        \n",
    "        Returns:\n",
    "        Tuple containing training and test data for Yelp reviews\n",
    "        \n",
    "    \"\"\"\n",
    "    yelp_file = 'data/yelp.csv'\n",
    "    yelp_df = pd.read_csv(yelp_file)\n",
    "    X, y = yelp_df.text, yelp_df.stars\n",
    "    \"\"\"\n",
    "        Split into train, test and validation set\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "    df = pd.concat([X_train, y_train], axis=1)\n",
    "    df_test = pd.concat([X_test, y_test], axis=1)\n",
    "    df_val = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "    \"\"\"\n",
    "        Save as csv file\n",
    "    \"\"\"\n",
    "    df.to_csv(r'data/yelp/train.csv')\n",
    "    df_test.to_csv(r'data/yelp/test.csv')\n",
    "    df_val.to_csv(r'data/yelp/dev.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Returns train test split for Amazon reviews\n",
    "\"\"\"\n",
    "def load_amz(test_size=0.20):\n",
    "    try:\n",
    "        with gzip.open(\"data/Amazon.pkl\", 'rb') as az:\n",
    "            reviews = pickle.load(az)\n",
    "            X, y = reviews.reviewText, reviews.overall\n",
    "            return train_test_split(X, y, test_size= test_size, random_state=1)\n",
    "    except FileNotFoundError:\n",
    "        \"\"\"\n",
    "            This block shouldn't execute since we should already have a  pickled file with Amazon reviews.\n",
    "            If the pickle file is missing for some reason, ensure that the Electronics_5.json is presnet.\n",
    "        \"\"\"\n",
    "        stream = pd.read_json('Electronics_5.json', lines = True, chunksize=10000)\n",
    "    \n",
    "        \"\"\"\n",
    "            Only extract reviews that are verified by Amazon and which have been voted helpful by users\n",
    "            Then filter out the review text and rating \n",
    "        \"\"\"\n",
    "\n",
    "        amazon_df = [df[(df.verified == True) & (df.vote.isna() == False)] \n",
    "                     for df in stream]\n",
    "        reviews_df = [df[['reviewText', 'overall']] for df in amazon_df[:150]]\n",
    "        reviews_df = pd.concat(reviews_df, sort=False)\n",
    "        \n",
    "        \"\"\"\n",
    "            Pickle the list so that we don't have extract the data again\n",
    "        \"\"\"\n",
    "        with gzip.open(\"data/Amazon.pkl\", \"wb\") as az:\n",
    "            pickle.dump(reviews_df, az)\n",
    "            X, y = reviews_df.reviewText, reviews_df.overall\n",
    "        return train_test_split(X, y, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "amz_train_review, amz_test_review, amz_train_ratings, amz_test_ratings = load_amz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_amazon_data():\n",
    "    f = gzip.open(\"data/Amazon.pkl\", 'rb')\n",
    "    reviews = pickle.load(f)\n",
    "    df = reviews[['reviewText', 'overall']]\n",
    "    df.loc[:, 'overall'] = df.overall.apply(lambda x : 1 if x == 4 or x == 5 else 0)\n",
    "    df_list = np.array_split(df, 5)\n",
    "    for idx, df in enumerate(df_list):\n",
    "        df.to_csv(f'data/Amazon/Amazon_set{idx+1}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_amazon_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/yelp_imbd/test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-181-07a58e038e7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'data/yelp_imdb/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'data/yelp_imbd/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_dev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'data/yelp_imbd/dev.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   1743\u001b[0m                                  \u001b[0mdoublequote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m                                  escapechar=escapechar, decimal=decimal)\n\u001b[0;32m-> 1745\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m             f, handles = _get_handle(self.path_or_buf, self.mode,\n\u001b[1;32m    155\u001b[0m                                      \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                                      compression=self.compression)\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;31m# Python 3 and encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;31m# Python 3 and no explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/yelp_imbd/test.csv'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_yelp_imbd():\n",
    "    \"\"\"\n",
    "        Read Yelp data and filter out only reviews and ratings\n",
    "    \"\"\"\n",
    "    \n",
    "    yelp_file = 'data/yelp.csv'\n",
    "    yelp_df = pd.read_csv(yelp_file)\n",
    "    yelp_df = yelp_df[['text', 'stars']]\n",
    "    yelp_df.loc[:, 'stars'] = yelp_df.stars.apply(lambda x : 1 if x == 4 or x == 5 else 0)\n",
    "    \n",
    "    \"\"\" \n",
    "        Read IMDB data and filter out reviews and ratings \n",
    "        Change columns name to match Yelp data since we will be combining the two datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    imdb_file = 'data/imdb.csv'\n",
    "    imdb_df = pd.read_csv(imdb_file)\n",
    "    imdb_df = imdb_df[['review', 'sentiment']]\n",
    "    imbd_df = imdb_df.rename(columns={'review':'text', 'sentiment':'stars'})\n",
    "    imbd_df.loc[:, 'stars'] = imbd_df.stars.apply(lambda x : 1 if x == \"positive\" else 0)\n",
    "\n",
    "    \"\"\"\n",
    "        Combine Yelp and IMDB reviews\n",
    "    \"\"\"\n",
    "    \n",
    "    yelp_imdb = pd.concat([yelp_df, imbd_df], axis=0)\n",
    "    X, y = yelp_imdb.text, yelp_imdb.stars\n",
    "    \n",
    "    \"\"\"\n",
    "        Split into train, test and validation set\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "    \n",
    "    \"\"\"\n",
    "        Set up dataframes for train, test and validation set\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.concat([X_train, y_train], axis=1)\n",
    "    df_test = pd.concat([X_test, y_test], axis=1)\n",
    "    df_dev = pd.concat([X_val, y_val], axis=1)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "        Save train, test and validation set as csv file\n",
    "    \"\"\"\n",
    "    df.to_csv(r'data/yelp_imdb/train.csv', index=False)\n",
    "    df_test.to_csv(r'data/yelp_imdb/test.csv', index=False)\n",
    "    df_dev.to_csv(r'data/yelp_imdb/dev.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_yelp_imbd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
