{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-09 19:56:10,297 loading file /home/prw0007/.flair/models/imdb.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/apps/rc/software/Anaconda3/5.3.1/envs/DeepNLP/lib/python3.6/site-packages/torch/serialization.py:573: DeprecationWarning: Call to deprecated class DocumentLSTMEmbeddings. (The functionality of this class is moved to 'DocumentRNNEmbeddings') -- Deprecated since version 0.4.\n",
      "  result = unpickler.load()\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import TextClassifier\n",
    "\n",
    "classifier = TextClassifier.load('en-sentiment') #English-sentiment model pre-trained with IMDB movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NEGATIVE (0.9706420302391052)]\n"
     ]
    }
   ],
   "source": [
    "sentence = Sentence('This film hurts. It is so bad that I am confused.')\n",
    "\n",
    "# predict NER tags\n",
    "classifier.predict(sentence)\n",
    "\n",
    "# print sentence with predicted labels\n",
    "print(sentence.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings\n",
    "\n",
    "# init embedding\n",
    "glove_embedding = WordEmbeddings('glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 1 The\n",
      "tensor([-0.0382, -0.2449,  0.7281, -0.3996,  0.0832,  0.0440, -0.3914,  0.3344,\n",
      "        -0.5755,  0.0875,  0.2879, -0.0673,  0.3091, -0.2638, -0.1323, -0.2076,\n",
      "         0.3340, -0.3385, -0.3174, -0.4834,  0.1464, -0.3730,  0.3458,  0.0520,\n",
      "         0.4495, -0.4697,  0.0263, -0.5415, -0.1552, -0.1411, -0.0397,  0.2828,\n",
      "         0.1439,  0.2346, -0.3102,  0.0862,  0.2040,  0.5262,  0.1716, -0.0824,\n",
      "        -0.7179, -0.4153,  0.2033, -0.1276,  0.4137,  0.5519,  0.5791, -0.3348,\n",
      "        -0.3656, -0.5486, -0.0629,  0.2658,  0.3020,  0.9977, -0.8048, -3.0243,\n",
      "         0.0125, -0.3694,  2.2167,  0.7220, -0.2498,  0.9214,  0.0345,  0.4674,\n",
      "         1.1079, -0.1936, -0.0746,  0.2335, -0.0521, -0.2204,  0.0572, -0.1581,\n",
      "        -0.3080, -0.4162,  0.3797,  0.1501, -0.5321, -0.2055, -1.2526,  0.0716,\n",
      "         0.7056,  0.4974, -0.4206,  0.2615, -1.5380, -0.3022, -0.0734, -0.2831,\n",
      "         0.3710, -0.2522,  0.0162, -0.0171, -0.3898,  0.8742, -0.7257, -0.5106,\n",
      "        -0.5203, -0.1459,  0.8278,  0.2706])\n",
      "Token: 2 grass\n",
      "tensor([-0.8135,  0.9404, -0.2405, -0.1350,  0.0557,  0.3363,  0.0802, -0.1015,\n",
      "        -0.5478, -0.3537,  0.0734,  0.2587,  0.1987, -0.1433,  0.2507,  0.4281,\n",
      "         0.1950,  0.5346,  0.7424,  0.0578, -0.3178,  0.9436,  0.8145, -0.0824,\n",
      "         0.6166,  0.7284, -0.3262, -1.3641,  0.1232,  0.5373, -0.5123,  0.0246,\n",
      "         1.0822, -0.2296,  0.6039,  0.5541, -0.9610,  0.4803,  0.0022,  0.5591,\n",
      "        -0.1637, -0.8468,  0.0741, -0.6216,  0.0260, -0.5162, -0.0525, -0.1418,\n",
      "        -0.0161, -0.4972, -0.5534, -0.4037,  0.5096,  1.0276, -0.0840, -1.1179,\n",
      "         0.3226,  0.4928,  0.9488,  0.2040,  0.5388,  0.8397, -0.0689,  0.3136,\n",
      "         1.0450, -0.2267, -0.0896, -0.6427,  0.6443, -1.1001, -0.0096,  0.2668,\n",
      "        -0.3230, -0.6065,  0.0479, -0.1664,  0.8571,  0.2335,  0.2539,  1.2546,\n",
      "         0.5472, -0.1980, -0.7186,  0.2076, -0.2587, -0.3650,  0.0834,  0.6932,\n",
      "         0.1574,  1.0931,  0.0913, -1.3773, -0.2717,  0.7071,  0.1872, -0.3307,\n",
      "        -0.2836,  0.1030,  1.2228,  0.8374])\n",
      "Token: 3 is\n",
      "tensor([-0.5426,  0.4148,  1.0322, -0.4024,  0.4669,  0.2182, -0.0749,  0.4733,\n",
      "         0.0810, -0.2208, -0.1281, -0.1144,  0.5089,  0.1157,  0.0282, -0.3628,\n",
      "         0.4382,  0.0475,  0.2028,  0.4986, -0.1007,  0.1327,  0.1697,  0.1165,\n",
      "         0.3135,  0.2571,  0.0928, -0.5683, -0.5297, -0.0515, -0.6733,  0.9253,\n",
      "         0.2693,  0.2273,  0.6636,  0.2622,  0.1972,  0.2609,  0.1877, -0.3454,\n",
      "        -0.4263,  0.1398,  0.5634, -0.5691,  0.1240, -0.1289,  0.7248, -0.2610,\n",
      "        -0.2631, -0.4360,  0.0789, -0.8415,  0.5160,  1.3997, -0.7646, -3.1453,\n",
      "        -0.2920, -0.3125,  1.5129,  0.5243,  0.2146,  0.4245, -0.0884, -0.1780,\n",
      "         1.1876,  0.1058,  0.7657,  0.2191,  0.3582, -0.1164,  0.0933, -0.6248,\n",
      "        -0.2190,  0.2180,  0.7406, -0.4374,  0.1434,  0.1472, -1.1605, -0.0505,\n",
      "         0.1268, -0.0144, -0.9868, -0.0913, -1.2054, -0.1197,  0.0478, -0.5400,\n",
      "         0.5246, -0.7096, -0.3253, -0.1346, -0.4131,  0.3343, -0.0072,  0.3225,\n",
      "        -0.0442, -1.2969,  0.7622,  0.4635])\n",
      "Token: 4 green\n",
      "tensor([-6.7907e-01,  3.4908e-01, -2.3984e-01, -9.9652e-01,  7.3782e-01,\n",
      "        -6.5911e-04,  2.8010e-01,  1.7287e-02, -3.6063e-01,  3.6955e-02,\n",
      "        -4.0395e-01,  2.4092e-02,  2.8958e-01,  4.0497e-01,  6.9992e-01,\n",
      "         2.5269e-01,  8.0350e-01,  4.9370e-02,  1.5562e-01, -6.3286e-03,\n",
      "        -2.9414e-01,  1.4728e-01,  1.8977e-01, -5.1791e-01,  3.6986e-01,\n",
      "         7.4582e-01,  8.2689e-02, -7.2601e-01, -4.0939e-01, -9.7822e-02,\n",
      "        -1.4096e-01,  7.1121e-01,  6.1933e-01, -2.5014e-01,  4.2250e-01,\n",
      "         4.8458e-01, -5.1915e-01,  7.7125e-01,  3.6685e-01,  4.9652e-01,\n",
      "        -4.1298e-02, -1.4683e+00,  2.0038e-01,  1.8591e-01,  4.9860e-02,\n",
      "        -1.7523e-01, -3.5528e-01,  9.4153e-01, -1.1898e-01, -5.1903e-01,\n",
      "        -1.1887e-02, -3.9186e-01, -1.7479e-01,  9.3451e-01, -5.8931e-01,\n",
      "        -2.7701e+00,  3.4522e-01,  8.6533e-01,  1.0808e+00, -1.0291e-01,\n",
      "        -9.1220e-02,  5.5092e-01, -3.9473e-01,  5.3676e-01,  1.0383e+00,\n",
      "        -4.0658e-01,  2.4590e-01, -2.6797e-01, -2.6036e-01, -1.4151e-01,\n",
      "        -1.2022e-01,  1.6234e-01, -7.4320e-01, -6.4728e-01,  4.7133e-02,\n",
      "         5.1642e-01,  1.9898e-01,  2.3919e-01,  1.2550e-01,  2.2471e-01,\n",
      "         8.2613e-01,  7.8328e-02, -5.7020e-01,  2.3934e-02, -1.5410e-01,\n",
      "        -2.5739e-01,  4.1262e-01, -4.6967e-01,  8.7914e-01,  7.2629e-01,\n",
      "         5.3862e-02, -1.1575e+00, -4.7835e-01,  2.0139e-01, -1.0051e+00,\n",
      "         1.1515e-01, -9.6609e-01,  1.2960e-01,  1.8388e-01, -3.0383e-02])\n",
      "Token: 5 .\n",
      "tensor([-0.3398,  0.2094,  0.4635, -0.6479, -0.3838,  0.0380,  0.1713,  0.1598,\n",
      "         0.4662, -0.0192,  0.4148, -0.3435,  0.2687,  0.0446,  0.4213, -0.4103,\n",
      "         0.1546,  0.0222, -0.6465,  0.2526,  0.0431, -0.1945,  0.4652,  0.4565,\n",
      "         0.6859,  0.0913,  0.2188, -0.7035,  0.1679, -0.3508, -0.1263,  0.6638,\n",
      "        -0.2582,  0.0365, -0.1361,  0.4025,  0.1429,  0.3813, -0.1228, -0.4589,\n",
      "        -0.2528, -0.3043, -0.1121, -0.2618, -0.2248, -0.4455,  0.2991, -0.8561,\n",
      "        -0.1450, -0.4909,  0.0083, -0.1749,  0.2752,  1.4401, -0.2124, -2.8435,\n",
      "        -0.2796, -0.4572,  1.6386,  0.7881, -0.5526,  0.6500,  0.0864,  0.3901,\n",
      "         1.0632, -0.3538,  0.4833,  0.3460,  0.8417,  0.0987, -0.2421, -0.2705,\n",
      "         0.0453, -0.4015,  0.1139,  0.0062,  0.0367,  0.0185, -1.0213, -0.2081,\n",
      "         0.6407, -0.0688, -0.5864,  0.3348, -1.1432, -0.1148, -0.2509, -0.4591,\n",
      "        -0.0968, -0.1795, -0.0634, -0.6741, -0.0689,  0.5360, -0.8777,  0.3180,\n",
      "        -0.3924, -0.2339,  0.4730, -0.0288])\n"
     ]
    }
   ],
   "source": [
    "# create sentence.\n",
    "sentence = Sentence('The grass is green .')\n",
    "\n",
    "# embed a sentence using glove.\n",
    "glove_embedding.embed(sentence)\n",
    "\n",
    "# now check out the embedded tokens.\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-09 19:19:25,633 Reading data from /home/prw0007/.flair/datasets/ud_english\n",
      "2019-11-09 19:19:25,634 Train: /home/prw0007/.flair/datasets/ud_english/en_ewt-ud-train.conllu\n",
      "2019-11-09 19:19:25,634 Test: /home/prw0007/.flair/datasets/ud_english/en_ewt-ud-test.conllu\n",
      "2019-11-09 19:19:25,635 Dev: /home/prw0007/.flair/datasets/ud_english/en_ewt-ud-dev.conllu\n",
      "12543\n",
      "2077\n",
      "2002\n",
      "Sentence: \"What if Google Morphed Into GoogleOS ?\" - 7 Tokens\n"
     ]
    }
   ],
   "source": [
    "import flair.datasets\n",
    "corpus = flair.datasets.UD_ENGLISH()\n",
    "\n",
    "# print the number of Sentences in the train split\n",
    "print(len(corpus.train))\n",
    "\n",
    "# print the number of Sentences in the test split\n",
    "print(len(corpus.test))\n",
    "\n",
    "# print the number of Sentences in the dev split\n",
    "print(len(corpus.dev))\n",
    "\n",
    "\n",
    "# print the first Sentence in the training split\n",
    "print(corpus.test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"TRAIN\": {\n",
      "        \"dataset\": \"TRAIN\",\n",
      "        \"total_number_of_documents\": 12543,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 204585,\n",
      "            \"min\": 1,\n",
      "            \"max\": 159,\n",
      "            \"avg\": 16.310691222195647\n",
      "        }\n",
      "    },\n",
      "    \"TEST\": {\n",
      "        \"dataset\": \"TEST\",\n",
      "        \"total_number_of_documents\": 2077,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 25096,\n",
      "            \"min\": 1,\n",
      "            \"max\": 81,\n",
      "            \"avg\": 12.082811747713048\n",
      "        }\n",
      "    },\n",
      "    \"DEV\": {\n",
      "        \"dataset\": \"DEV\",\n",
      "        \"total_number_of_documents\": 2002,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 25148,\n",
      "            \"min\": 1,\n",
      "            \"max\": 75,\n",
      "            \"avg\": 12.561438561438562\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "stats = corpus.obtain_statistics()\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import CSVClassificationCorpus\n",
    "\n",
    "##Note: You will need to save your split CSV data files in the data_folder path with each file titled appropriately \n",
    "#       i.e. train.csv test.csv dev.csv. This is because the corpus initializers will automatically search for the \n",
    "#       train, dev, test splits in a folder.\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = '/path/to/data'\n",
    "\n",
    "# column format indicating which columns hold the text and label(s)\n",
    "column_name_map = {4: \"text\", 1: \"label_topic\", 2: \"label_subtopic\"}\n",
    "\n",
    "# load corpus containing training, test and dev data and if CSV has a header, you can skip it\n",
    "corpus: Corpus = CSVClassificationCorpus(data_folder,\n",
    "                                         column_name_map,\n",
    "                                         skip_header=True,\n",
    "                                         delimiter='\\t',    # tab-separated files\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-09 19:55:33,428 Reading data from /home/prw0007/.flair/datasets/imdb\n",
      "2019-11-09 19:55:33,430 Train: /home/prw0007/.flair/datasets/imdb/train.txt\n",
      "2019-11-09 19:55:33,432 Dev: None\n",
      "2019-11-09 19:55:33,434 Test: None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'exists'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bbc1a80b93e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 1. get the corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcorpus\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIMDB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/rc/software/Anaconda3/5.3.1/envs/DeepNLP/lib/python3.6/site-packages/flair/datasets.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, base_path, in_memory)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m         super(IMDB, self).__init__(\n\u001b[0;32m--> 845\u001b[0;31m             \u001b[0mdata_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_tokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m         )\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/rc/software/Anaconda3/5.3.1/envs/DeepNLP/lib/python3.6/site-packages/flair/datasets.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_folder, train_file, test_file, dev_file, use_tokenizer, max_tokens_per_doc, in_memory)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0muse_tokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0mmax_tokens_per_doc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_tokens_per_doc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0min_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         )\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/rc/software/Anaconda3/5.3.1/envs/DeepNLP/lib/python3.6/site-packages/flair/datasets.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_to_file, max_tokens_per_doc, use_tokenizer, in_memory)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mpath_to_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mpath_to_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"__label__\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'exists'"
     ]
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import IMDB\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentRNNEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "# 1. get the corpus\n",
    "corpus: Corpus = IMDB().downsample(0.1)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. create the label dictionary\n",
    "label_dict = corpus.make_label_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'<unk>', b'O', b'S-person', b'S-corporation', b'B-product', b'I-product', b'E-product', b'S-location', b'B-location', b'E-location', b'B-person', b'E-person', b'S-product', b'S-group', b'I-person', b'B-corporation', b'E-corporation', b'B-creative-work', b'I-creative-work', b'E-creative-work', b'B-group', b'E-group', b'I-location', b'I-group', b'S-creative-work', b'<START>', b'<STOP>']\n"
     ]
    }
   ],
   "source": [
    "# 3. make a list of word embeddings\n",
    "word_embeddings = [WordEmbeddings('glove'),\n",
    "\n",
    "                   # comment in flair embeddings for state-of-the-art results\n",
    "                   # FlairEmbeddings('news-forward'),\n",
    "                   # FlairEmbeddings('news-backward'),\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/apps/rc/software/Anaconda3/5.3.1/envs/DeepNLP/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "# 4. initialize document embedding by passing list of word embeddings\n",
    "# Can choose between many RNN types (GRU by default, to change use rnn_type parameter)\n",
    "document_embeddings: DocumentRNNEmbeddings = DocumentRNNEmbeddings(word_embeddings,\n",
    "                                                                     hidden_size=512,\n",
    "                                                                     reproject_words=True,\n",
    "                                                                     reproject_words_dimension=256,\n",
    "                                                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. create the text classifier\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. initialize the text classifier trainer\n",
    "trainer = ModelTrainer(classifier, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-09 19:42:30,042 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-09 19:42:30,043 Evaluation method: MICRO_F1_SCORE\n",
      "2019-11-09 19:42:30,435 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-09 19:42:31,020 epoch 1 - iter 0/11 - loss 4.25442219\n",
      "2019-11-09 19:42:31,449 epoch 1 - iter 1/11 - loss 3.96355748\n",
      "2019-11-09 19:42:31,886 epoch 1 - iter 2/11 - loss 4.22805897\n",
      "2019-11-09 19:42:32,425 epoch 1 - iter 3/11 - loss 4.35936630\n",
      "2019-11-09 19:42:32,828 epoch 1 - iter 4/11 - loss 4.79208441\n",
      "2019-11-09 19:42:33,334 epoch 1 - iter 5/11 - loss 4.68821247\n",
      "2019-11-09 19:42:33,824 epoch 1 - iter 6/11 - loss 5.01159436\n",
      "2019-11-09 19:42:34,277 epoch 1 - iter 7/11 - loss 4.82517886\n",
      "2019-11-09 19:42:34,801 epoch 1 - iter 8/11 - loss 4.98456658\n",
      "2019-11-09 19:42:35,262 epoch 1 - iter 9/11 - loss 4.96149368\n",
      "2019-11-09 19:42:35,655 epoch 1 - iter 10/11 - loss 4.89703738\n",
      "2019-11-09 19:42:36,231 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-09 19:42:36,234 EPOCH 1 done: loss 4.8970 - lr 0.1000 - bad epochs 0\n",
      "2019-11-09 19:42:37,164 DEV : loss 6.965199947357178 - score 0.0\n",
      "2019-11-09 19:42:38,354 TEST : loss 7.478843688964844 - score 0.0\n",
      "2019-11-09 19:42:42,178 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-09 19:42:42,794 epoch 2 - iter 0/11 - loss 2.61735058\n",
      "2019-11-09 19:42:43,362 epoch 2 - iter 1/11 - loss 3.73651338\n",
      "2019-11-09 19:42:43,925 epoch 2 - iter 2/11 - loss 3.49885496\n",
      "2019-11-09 19:42:44,412 epoch 2 - iter 3/11 - loss 4.10886168\n",
      "2019-11-09 19:42:44,980 epoch 2 - iter 4/11 - loss 4.37210884\n",
      "2019-11-09 19:42:45,455 epoch 2 - iter 5/11 - loss 4.67094278\n",
      "2019-11-09 19:42:45,913 epoch 2 - iter 6/11 - loss 4.37775816\n",
      "2019-11-09 19:42:46,378 epoch 2 - iter 7/11 - loss 4.43886536\n",
      "2019-11-09 19:42:46,826 epoch 2 - iter 8/11 - loss 4.70674753\n",
      "2019-11-09 19:42:47,243 epoch 2 - iter 9/11 - loss 4.70636935\n",
      "2019-11-09 19:42:47,643 epoch 2 - iter 10/11 - loss 4.74106273\n",
      "2019-11-09 19:42:48,205 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-09 19:42:48,208 EPOCH 2 done: loss 4.7411 - lr 0.1000 - bad epochs 0\n",
      "2019-11-09 19:42:49,130 DEV : loss 6.799211025238037 - score 0.0\n",
      "2019-11-09 19:42:50,320 TEST : loss 7.40789270401001 - score 0.0\n",
      "2019-11-09 19:42:54,135 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-09 19:42:54,748 epoch 3 - iter 0/11 - loss 2.63428926\n",
      "2019-11-09 19:42:55,237 epoch 3 - iter 1/11 - loss 3.97606444\n",
      "2019-11-09 19:42:55,745 epoch 3 - iter 2/11 - loss 4.74336513\n",
      "2019-11-09 19:42:56,154 epoch 3 - iter 3/11 - loss 4.17434448\n",
      "2019-11-09 19:42:56,568 epoch 3 - iter 4/11 - loss 4.27017312\n",
      "2019-11-09 19:42:56,962 epoch 3 - iter 5/11 - loss 4.66261248\n",
      "2019-11-09 19:42:57,370 epoch 3 - iter 6/11 - loss 4.51767847\n",
      "2019-11-09 19:42:57,804 epoch 3 - iter 7/11 - loss 4.53470165\n",
      "2019-11-09 19:42:58,215 epoch 3 - iter 8/11 - loss 4.49615261\n",
      "2019-11-09 19:42:58,660 epoch 3 - iter 9/11 - loss 4.65090609\n",
      "2019-11-09 19:42:59,106 epoch 3 - iter 10/11 - loss 4.58408152\n",
      "2019-11-09 19:42:59,675 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-09 19:42:59,678 EPOCH 3 done: loss 4.5841 - lr 0.1000 - bad epochs 1\n",
      "2019-11-09 19:43:00,598 DEV : loss 7.480216979980469 - score 0.0\n",
      "2019-11-09 19:43:01,796 TEST : loss 8.007878303527832 - score 0.0\n",
      "2019-11-09 19:43:05,572 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-09 19:43:06,213 epoch 4 - iter 0/11 - loss 5.78925514\n",
      "2019-11-09 19:43:06,779 epoch 4 - iter 1/11 - loss 4.66392708\n",
      "2019-11-09 19:43:07,231 epoch 4 - iter 2/11 - loss 4.11099768\n",
      "2019-11-09 19:43:07,873 epoch 4 - iter 3/11 - loss 3.81118608\n",
      "2019-11-09 19:43:08,276 epoch 4 - iter 4/11 - loss 3.72102990\n",
      "2019-11-09 19:43:08,660 epoch 4 - iter 5/11 - loss 4.07370698\n",
      "2019-11-09 19:43:09,160 epoch 4 - iter 6/11 - loss 4.08900033\n",
      "2019-11-09 19:43:09,595 epoch 4 - iter 7/11 - loss 4.39184979\n",
      "2019-11-09 19:43:10,054 epoch 4 - iter 8/11 - loss 4.47976147\n",
      "2019-11-09 19:43:10,490 epoch 4 - iter 9/11 - loss 4.53330090\n",
      "2019-11-09 19:43:10,952 epoch 4 - iter 10/11 - loss 4.76165856\n",
      "2019-11-09 19:43:11,595 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-09 19:43:11,598 EPOCH 4 done: loss 4.7617 - lr 0.1000 - bad epochs 2\n",
      "2019-11-09 19:43:12,525 DEV : loss 6.406219959259033 - score 0.0\n",
      "2019-11-09 19:43:13,734 TEST : loss 6.937859535217285 - score 0.0\n",
      "2019-11-09 19:43:17,546 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-09 19:43:18,204 epoch 5 - iter 0/11 - loss 6.14440680\n",
      "2019-11-09 19:43:18,675 epoch 5 - iter 1/11 - loss 5.55887532\n",
      "2019-11-09 19:43:20,417 epoch 5 - iter 2/11 - loss 4.72506650\n",
      "2019-11-09 19:43:21,185 epoch 5 - iter 3/11 - loss 4.58243310\n",
      "2019-11-09 19:43:21,630 epoch 5 - iter 4/11 - loss 4.43355322\n",
      "2019-11-09 19:43:22,115 epoch 5 - iter 5/11 - loss 4.27501949\n",
      "2019-11-09 19:43:22,719 epoch 5 - iter 6/11 - loss 4.36755398\n",
      "2019-11-09 19:43:23,225 epoch 5 - iter 7/11 - loss 4.32203567\n",
      "2019-11-09 19:43:23,612 epoch 5 - iter 8/11 - loss 4.46542719\n",
      "2019-11-09 19:43:24,170 epoch 5 - iter 9/11 - loss 4.51365256\n",
      "2019-11-09 19:43:24,701 epoch 5 - iter 10/11 - loss 4.45869073\n",
      "2019-11-09 19:43:25,393 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-09 19:43:25,396 EPOCH 5 done: loss 4.4587 - lr 0.1000 - bad epochs 3\n",
      "2019-11-09 19:43:26,317 DEV : loss 6.934731483459473 - score 0.0\n",
      "2019-11-09 19:43:27,507 TEST : loss 7.502326965332031 - score 0.0\n",
      "Epoch     4: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2019-11-09 19:43:31,271 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-09 19:43:31,902 epoch 6 - iter 0/11 - loss 4.33209181\n",
      "2019-11-09 19:43:32,353 epoch 6 - iter 1/11 - loss 5.31028581\n",
      "2019-11-09 19:43:32,751 epoch 6 - iter 2/11 - loss 4.55757078\n",
      "2019-11-09 19:43:33,139 epoch 6 - iter 3/11 - loss 4.51954067\n",
      "2019-11-09 19:43:33,546 epoch 6 - iter 4/11 - loss 4.36547437\n",
      "2019-11-09 19:43:33,911 epoch 6 - iter 5/11 - loss 4.12307894\n",
      "2019-11-09 19:43:34,346 epoch 6 - iter 6/11 - loss 4.29107056\n",
      "2019-11-09 19:43:34,735 epoch 6 - iter 7/11 - loss 4.06694824\n",
      "2019-11-09 19:43:35,112 epoch 6 - iter 8/11 - loss 4.35182587\n",
      "2019-11-09 19:43:35,547 epoch 6 - iter 9/11 - loss 4.28338995\n",
      "2019-11-09 19:43:35,977 epoch 6 - iter 10/11 - loss 4.45839544\n",
      "2019-11-09 19:43:36,554 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-09 19:43:36,557 EPOCH 6 done: loss 4.4584 - lr 0.0500 - bad epochs 0\n",
      "2019-11-09 19:43:37,477 DEV : loss 6.240056991577148 - score 0.0227\n",
      "2019-11-09 19:43:38,670 TEST : loss 6.772904396057129 - score 0.0\n",
      "2019-11-09 19:43:42,432 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-09 19:43:43,064 epoch 7 - iter 0/11 - loss 3.58253527\n",
      "2019-11-09 19:43:43,566 epoch 7 - iter 1/11 - loss 2.98587108\n",
      "2019-11-09 19:43:44,053 epoch 7 - iter 2/11 - loss 3.73563401\n",
      "2019-11-09 19:43:44,521 epoch 7 - iter 3/11 - loss 4.09041202\n",
      "2019-11-09 19:43:45,018 epoch 7 - iter 4/11 - loss 3.85096207\n",
      "2019-11-09 19:43:45,432 epoch 7 - iter 5/11 - loss 4.42480548\n",
      "2019-11-09 19:43:45,941 epoch 7 - iter 6/11 - loss 4.16045509\n",
      "2019-11-09 19:43:46,404 epoch 7 - iter 7/11 - loss 4.14559752\n",
      "2019-11-09 19:43:46,764 epoch 7 - iter 8/11 - loss 4.16379171\n",
      "2019-11-09 19:43:47,160 epoch 7 - iter 9/11 - loss 4.25228472\n",
      "2019-11-09 19:43:47,531 epoch 7 - iter 10/11 - loss 4.45440492\n",
      "2019-11-09 19:43:48,118 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-09 19:43:48,121 EPOCH 7 done: loss 4.4544 - lr 0.0500 - bad epochs 0\n",
      "2019-11-09 19:43:49,042 DEV : loss 6.4082231521606445 - score 0.0227\n",
      "2019-11-09 19:43:50,260 TEST : loss 6.989791393280029 - score 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-09 19:43:54,068 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-09 19:43:54,690 epoch 8 - iter 0/11 - loss 1.49203074\n",
      "2019-11-09 19:43:55,149 epoch 8 - iter 1/11 - loss 4.00940067\n",
      "2019-11-09 19:43:55,582 epoch 8 - iter 2/11 - loss 3.73690037\n",
      "2019-11-09 19:43:56,103 epoch 8 - iter 3/11 - loss 4.31594482\n",
      "2019-11-09 19:43:56,618 epoch 8 - iter 4/11 - loss 4.12532446\n",
      "2019-11-09 19:43:57,059 epoch 8 - iter 5/11 - loss 4.31468823\n",
      "2019-11-09 19:43:57,454 epoch 8 - iter 6/11 - loss 4.31709325\n",
      "2019-11-09 19:43:57,863 epoch 8 - iter 7/11 - loss 4.34334321\n",
      "2019-11-09 19:43:58,317 epoch 8 - iter 8/11 - loss 4.23089108\n",
      "2019-11-09 19:43:58,846 epoch 8 - iter 9/11 - loss 4.48738385\n",
      "2019-11-09 19:43:59,315 epoch 8 - iter 10/11 - loss 4.40320122\n",
      "2019-11-09 19:43:59,985 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-09 19:43:59,987 EPOCH 8 done: loss 4.4032 - lr 0.0500 - bad epochs 1\n",
      "2019-11-09 19:44:00,978 DEV : loss 6.317799091339111 - score 0.0227\n",
      "2019-11-09 19:44:02,186 TEST : loss 6.908603191375732 - score 0.0\n",
      "2019-11-09 19:44:05,987 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-09 19:44:06,632 epoch 9 - iter 0/11 - loss 7.76299572\n",
      "2019-11-09 19:44:07,028 epoch 9 - iter 1/11 - loss 6.37082863\n",
      "2019-11-09 19:44:07,424 epoch 9 - iter 2/11 - loss 5.37186019\n",
      "2019-11-09 19:44:07,827 epoch 9 - iter 3/11 - loss 4.76098603\n",
      "2019-11-09 19:44:08,225 epoch 9 - iter 4/11 - loss 4.88598151\n",
      "2019-11-09 19:44:08,591 epoch 9 - iter 5/11 - loss 4.61050804\n",
      "2019-11-09 19:44:08,962 epoch 9 - iter 6/11 - loss 4.53281879\n",
      "2019-11-09 19:44:09,387 epoch 9 - iter 7/11 - loss 4.29111648\n",
      "2019-11-09 19:44:09,821 epoch 9 - iter 8/11 - loss 4.29415369\n",
      "2019-11-09 19:44:10,204 epoch 9 - iter 9/11 - loss 4.26999836\n",
      "2019-11-09 19:44:10,609 epoch 9 - iter 10/11 - loss 4.35245163\n",
      "2019-11-09 19:44:11,149 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-09 19:44:11,152 EPOCH 9 done: loss 4.3525 - lr 0.0500 - bad epochs 2\n",
      "2019-11-09 19:44:12,109 DEV : loss 6.247194290161133 - score 0.0227\n",
      "2019-11-09 19:44:13,334 TEST : loss 6.874978542327881 - score 0.0\n",
      "2019-11-09 19:44:17,109 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-09 19:44:17,761 epoch 10 - iter 0/11 - loss 6.19703770\n",
      "2019-11-09 19:44:18,216 epoch 10 - iter 1/11 - loss 5.96217108\n",
      "2019-11-09 19:44:18,663 epoch 10 - iter 2/11 - loss 5.62669977\n",
      "2019-11-09 19:44:19,212 epoch 10 - iter 3/11 - loss 4.88167757\n",
      "2019-11-09 19:44:19,704 epoch 10 - iter 4/11 - loss 4.72502494\n",
      "2019-11-09 19:44:20,321 epoch 10 - iter 5/11 - loss 4.47628140\n",
      "2019-11-09 19:44:20,754 epoch 10 - iter 6/11 - loss 4.14251075\n",
      "2019-11-09 19:44:21,429 epoch 10 - iter 7/11 - loss 4.08021343\n",
      "2019-11-09 19:44:21,983 epoch 10 - iter 8/11 - loss 4.23653295\n",
      "2019-11-09 19:44:22,448 epoch 10 - iter 9/11 - loss 4.18813353\n",
      "2019-11-09 19:44:22,926 epoch 10 - iter 10/11 - loss 4.26467193\n",
      "2019-11-09 19:44:23,536 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-09 19:44:23,539 EPOCH 10 done: loss 4.2647 - lr 0.0500 - bad epochs 3\n",
      "2019-11-09 19:44:24,470 DEV : loss 6.207584857940674 - score 0.0227\n",
      "2019-11-09 19:44:25,679 TEST : loss 6.840757846832275 - score 0.0\n",
      "Epoch     9: reducing learning rate of group 0 to 2.5000e-02.\n",
      "2019-11-09 19:44:33,272 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-09 19:44:33,274 Testing using best model ...\n",
      "2019-11-09 19:44:33,277 loading file resources/taggers/example-ner/best-model.pt\n",
      "2019-11-09 19:44:35,489 0.0\t0.0\t0.0\n",
      "2019-11-09 19:44:35,492 \n",
      "MICRO_AVG: acc 0.0 - f1-score 0.0\n",
      "MACRO_AVG: acc 0.0 - f1-score 0.0\n",
      "corporation tp: 0 - fp: 0 - fn: 5 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "creative-work tp: 0 - fp: 0 - fn: 21 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "group      tp: 0 - fp: 0 - fn: 26 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "location   tp: 0 - fp: 0 - fn: 9 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "person     tp: 0 - fp: 0 - fn: 53 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "product    tp: 0 - fp: 0 - fn: 13 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "2019-11-09 19:44:35,494 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.0,\n",
       " 'dev_score_history': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0227,\n",
       "  0.0227,\n",
       "  0.0227,\n",
       "  0.0227,\n",
       "  0.0227],\n",
       " 'train_loss_history': [4.897037376057018,\n",
       "  4.741062727841464,\n",
       "  4.584081519733775,\n",
       "  4.761658560145985,\n",
       "  4.458690730008212,\n",
       "  4.458395437760786,\n",
       "  4.454404917630282,\n",
       "  4.403201222419739,\n",
       "  4.352451627904719,\n",
       "  4.26467193256725],\n",
       " 'dev_loss_history': [tensor(6.9652, device='cuda:0'),\n",
       "  tensor(6.7992, device='cuda:0'),\n",
       "  tensor(7.4802, device='cuda:0'),\n",
       "  tensor(6.4062, device='cuda:0'),\n",
       "  tensor(6.9347, device='cuda:0'),\n",
       "  tensor(6.2401, device='cuda:0'),\n",
       "  tensor(6.4082, device='cuda:0'),\n",
       "  tensor(6.3178, device='cuda:0'),\n",
       "  tensor(6.2472, device='cuda:0'),\n",
       "  tensor(6.2076, device='cuda:0')]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. start the training\n",
    "trainer.train('resources/taggers/ag_news',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              anneal_factor=0.5,\n",
    "              patience=5,\n",
    "              max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. plot weight traces (optional)\n",
    "from flair.visual.training_curves import Plotter\n",
    "plotter = Plotter()\n",
    "plotter.plot_weights('resources/taggers/ag_news/weights.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-09 19:45:24,662 loading file resources/taggers/example-ner/final-model.pt\n",
      "I love Berlin\n"
     ]
    }
   ],
   "source": [
    "classifier = TextClassifier.load('resources/taggers/ag_news/final-model.pt')\n",
    "\n",
    "# create example sentence\n",
    "sentence = Sentence('France is the current world cup winner.')\n",
    "\n",
    "# predict class and print\n",
    "classifier.predict(sentence)\n",
    "\n",
    "print(sentence.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load/Use IMDB-Trained Sentiment Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-09 19:59:49,339 loading file /home/prw0007/.flair/models/imdb.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/apps/rc/software/Anaconda3/5.3.1/envs/DeepNLP/lib/python3.6/site-packages/torch/serialization.py:573: DeprecationWarning: Call to deprecated class DocumentLSTMEmbeddings. (The functionality of this class is moved to 'DocumentRNNEmbeddings') -- Deprecated since version 0.4.\n",
      "  result = unpickler.load()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[POSITIVE (0.9321351051330566)]\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import TextClassifier\n",
    "\n",
    "classifier = TextClassifier.load('/home/prw0007/.flair/models/imdb.pt')\n",
    "\n",
    "# create example sentence\n",
    "sentence = Sentence('France is the current world cup winner.')\n",
    "\n",
    "# predict class and print\n",
    "classifier.predict(sentence)\n",
    "\n",
    "print(sentence.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DeepNLP]",
   "language": "python",
   "name": "conda-env-DeepNLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
