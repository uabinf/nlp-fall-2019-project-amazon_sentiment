{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Domain Sentiment Analysis Study\n",
    "NLP Group Project\n",
    "\n",
    "Atulya Shetty, Payton Walker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Returns train test split for Amazon reviews\n",
    "\"\"\"\n",
    "def load_amz(test_size=0.20):\n",
    "    try:\n",
    "        with gzip.open(\"data/final/Amazon.pkl\", 'rb') as az:\n",
    "            reviews = pickle.load(az)\n",
    "            X, y = reviews.reviewText, reviews.overall\n",
    "            return train_test_split(X, y, test_size= test_size, random_state=1)\n",
    "    except FileNotFoundError:\n",
    "        \"\"\"\n",
    "            This block shouldn't execute since we should already have a  pickled file with Amazon reviews.\n",
    "            If the pickle file is missing for some reason, ensure that the Electronics_5.json is presenet \n",
    "            in data/final.\n",
    "        \"\"\"\n",
    "        stream = pd.read_json('Electronics_5.json', lines = True, chunksize=10000)\n",
    "    \n",
    "        \"\"\"\n",
    "            Only extract reviews that are verified by Amazon and which have been voted helpful by users\n",
    "            Then filter out the review text and rating \n",
    "        \"\"\"\n",
    "\n",
    "        amazon_df = [df[(df.verified == True) & (df.vote.isna() == False)] \n",
    "                     for df in stream]\n",
    "        reviews_df = [df[['reviewText', 'overall']] for df in amazon_df[:150]]\n",
    "        reviews_df = pd.concat(reviews_df, sort=False)\n",
    "        \n",
    "        \"\"\"\n",
    "            Pickle the list so that we don't have extract the data again\n",
    "        \"\"\"\n",
    "        with gzip.open(\"data/final/Amazon.pkl\", \"wb\") as az:\n",
    "            pickle.dump(reviews_df, az)\n",
    "            X, y = reviews_df.reviewText, reviews_df.overall\n",
    "        return train_test_split(X, y, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_amazon_data():\n",
    "    f = gzip.open(\"data/final/Amazon.pkl\", 'rb')\n",
    "    reviews = pickle.load(f)\n",
    "    df = reviews[['reviewText', 'overall']]\n",
    "    df.loc[:, 'overall'] = df.overall.apply(lambda x : 1 if x == 4 or x == 5 else 0)\n",
    "    df_pos = df.loc[df['overall'] == 1][:10000]\n",
    "    df_neg = df.loc[df['overall'] == 0][:10000]\n",
    "    df_comb = pd.concat([df_pos[:5000], df_neg[:5000]], axis=0)\n",
    "    df_comb2 = pd.concat([df_pos[5000:], df_neg[5000:]], axis=0)\n",
    "    df_comb = shuffle(df_comb, random_state=12)\n",
    "    df_comb2 = shuffle(df_comb2, random_state=12)\n",
    "\n",
    "    \n",
    "    df_comb.to_csv(r'data/S1/test.csv', index=False)\n",
    "    df_comb.to_csv(r'data/S2/test.csv', index=False)\n",
    "    df_comb.to_csv(r'data/S3/test.csv', index=False)\n",
    "    df_comb.to_csv(r'data/S4/test.csv', index=False)\n",
    "\n",
    "split_amazon_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_data():\n",
    "    imdb_file = 'data/final/imdb.csv'\n",
    "    imdb_df = pd.read_csv(imdb_file)\n",
    "    X, y = imdb_df.review, imdb_df.sentiment\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)    \n",
    "    df = pd.concat([X_train, y_train], axis=1)\n",
    "    df_dev = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    \"\"\"\n",
    "         Save as csv file\n",
    "    \"\"\"\n",
    "    df.to_csv(r'data/S1/train.csv', index=False)\n",
    "    df_dev.to_csv(r'data/S1/dev.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yelp_data():\n",
    "    \n",
    "    \"\"\" \n",
    "        Parameters:\n",
    "        test_size = float, optional (default=0.2)\n",
    "        \n",
    "        Returns:\n",
    "        Tuple containing training and test data for Yelp reviews\n",
    "        \n",
    "    \"\"\"\n",
    "    yelp_file = 'data/final/yelp.csv'\n",
    "    yelp_df = pd.read_csv(yelp_file)\n",
    "    yelp_df.loc[:, 'stars'] = yelp_df.stars.apply(lambda x : 1 if x == 4 or x == 5 else 0)\n",
    "    yelp_df = yelp_df[['text', 'stars']]\n",
    "    X, y = yelp_df.text, yelp_df.stars\n",
    "    \"\"\"\n",
    "        Split into train, test and validation set\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)    \n",
    "    df = pd.concat([X_train, y_train], axis=1)\n",
    "    df_dev = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    \"\"\"\n",
    "        Save as csv file\n",
    "    \"\"\"\n",
    "    df.to_csv(r'data/S2/train.csv', index=False)\n",
    "    df_dev.to_csv(r'data/S2/dev.csv', index=False)\n",
    "    print(df.shape)\n",
    "    print(df_dev.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_yelp_imbd():\n",
    "    \"\"\"\n",
    "        Read Yelp data and filter out only reviews and ratings\n",
    "    \"\"\"\n",
    "    \n",
    "    yelp_file = 'data/final/yelp.csv'\n",
    "    yelp_df = pd.read_csv(yelp_file)\n",
    "    yelp_df = yelp_df[['text', 'stars']]\n",
    "    yelp_df.loc[:, 'stars'] = yelp_df.stars.apply(lambda x : 1 if x == 4 or x == 5 else 0)\n",
    "    \n",
    "    \"\"\" \n",
    "        Read IMDB data and filter out reviews and ratings \n",
    "        Change columns name to match Yelp data since we will be combining the two datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    imdb_file = 'data/final/imdb.csv'\n",
    "    imdb_df = pd.read_csv(imdb_file)\n",
    "    imdb_df = imdb_df[['review', 'sentiment']]\n",
    "    imbd_df = imdb_df.rename(columns={'review':'text', 'sentiment':'stars'})\n",
    "    imbd_df.loc[:, 'stars'] = imbd_df.stars.apply(lambda x : 1 if x == \"positive\" else 0)\n",
    "\n",
    "    \"\"\"\n",
    "        Combine Yelp and IMDB reviews\n",
    "    \"\"\"\n",
    "    \n",
    "    yelp_imdb = pd.concat([yelp_df, imbd_df], axis=0)\n",
    "    X, y = yelp_imdb.text, yelp_imdb.stars\n",
    "    \n",
    "    \"\"\"\n",
    "        Split into train, test and validation set\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)    \n",
    "    \n",
    "    \"\"\"\n",
    "        Set up dataframes for train, test and validation set\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.concat([X_train, y_train], axis=1)\n",
    "    df_dev = pd.concat([X_test, y_test], axis=1)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "        Save train, test and validation set as csv file\n",
    "    \"\"\"\n",
    "    df.to_csv(r'data/S3/train.csv', index=False)\n",
    "    df_dev.to_csv(r'data/S3/dev.csv', index=False)\n",
    "    print(df.shape)\n",
    "    print(df_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 2)\n",
      "(12000, 2)\n"
     ]
    }
   ],
   "source": [
    "combine_yelp_imbd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_yelp_imdb_amz():\n",
    "    \"\"\"\n",
    "        Read Yelp data and filter out only reviews and ratings\n",
    "    \"\"\"\n",
    "\n",
    "    yelp_file = 'data/final/yelp.csv'\n",
    "    yelp_df = pd.read_csv(yelp_file)\n",
    "    yelp_df = yelp_df[['text', 'stars']]\n",
    "    yelp_df.loc[:, 'stars'] = yelp_df.stars.apply(lambda x : 1 if x == 4 or x == 5 else 0)\n",
    "\n",
    "    \"\"\" \n",
    "        Read IMDB data and filter out reviews and ratings \n",
    "        Change columns name to match Yelp data since we will be combining the two datasets\n",
    "    \"\"\"\n",
    "\n",
    "    imdb_file = 'data/final/imdb.csv'\n",
    "    imdb_df = pd.read_csv(imdb_file)\n",
    "    imdb_df = imdb_df[['review', 'sentiment']]\n",
    "    imbd_df = imdb_df.rename(columns={'review':'text', 'sentiment':'stars'})\n",
    "    imbd_df.loc[:, 'stars'] = imbd_df.stars.apply(lambda x : 1 if x == \"positive\" else 0)\n",
    "\n",
    "\n",
    "    f = gzip.open(\"data/final/Amazon.pkl\", 'rb')\n",
    "    reviews = pickle.load(f)\n",
    "    df = reviews[['reviewText', 'overall']]\n",
    "    df.loc[:, 'overall'] = df.overall.apply(lambda x : 1 if x == 4 or x == 5 else 0)\n",
    "    df_pos = df.loc[df['overall'] == 1][10000:]\n",
    "    df_neg = df.loc[df['overall'] == 0][10000:]\n",
    "    df_comb = pd.concat([df_pos[:10000], df_neg[:10000]], axis=0)\n",
    "    df_comb = shuffle(df_comb, random_state=12)\n",
    "    df_comb_az = df_comb.rename(columns={'reviewText':'text', 'overall':'stars'})\n",
    "\n",
    "    df_ayi = pd.concat([imbd_df, df_comb_az, yelp_df ], axis=0, sort=False)\n",
    "\n",
    "    X, y = df_ayi.text, df_ayi.stars\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)    \n",
    "    df = pd.concat([X_train, y_train], axis=1)\n",
    "    df_dev = pd.concat([X_test, y_test], axis=1)\n",
    "    df.to_csv(r'data/S4/train.csv', index=False)\n",
    "    df_dev.to_csv(r'data/S4/dev.csv', index=False)\n",
    "    print(df.shape)\n",
    "    print(df_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64000, 2)\n",
      "(16000, 2)\n"
     ]
    }
   ],
   "source": [
    "combine_yelp_imdb_amz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
