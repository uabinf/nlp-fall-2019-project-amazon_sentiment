{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Domain Sentiment Analysis Study\n",
    "NLP Group Project\n",
    "\n",
    "Atulya Shetty, Payton Walker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Returns train test split for Amazon reviews\n",
    "\"\"\"\n",
    "def load_amz(test_size=0.20):\n",
    "    try:\n",
    "        with gzip.open(\"data/final/Amazon.pkl\", 'rb') as az:\n",
    "            reviews = pickle.load(az)\n",
    "            X, y = reviews.reviewText, reviews.overall\n",
    "            return train_test_split(X, y, test_size= test_size, random_state=1)\n",
    "    except FileNotFoundError:\n",
    "        \"\"\"\n",
    "            This block shouldn't execute since we should already have a  pickled file with Amazon reviews.\n",
    "            If the pickle file is missing for some reason, ensure that the Electronics_5.json is presnet.\n",
    "        \"\"\"\n",
    "        stream = pd.read_json('Electronics_5.json', lines = True, chunksize=10000)\n",
    "    \n",
    "        \"\"\"\n",
    "            Only extract reviews that are verified by Amazon and which have been voted helpful by users\n",
    "            Then filter out the review text and rating \n",
    "        \"\"\"\n",
    "\n",
    "        amazon_df = [df[(df.verified == True) & (df.vote.isna() == False)] \n",
    "                     for df in stream]\n",
    "        reviews_df = [df[['reviewText', 'overall']] for df in amazon_df[:150]]\n",
    "        reviews_df = pd.concat(reviews_df, sort=False)\n",
    "        \n",
    "        \"\"\"\n",
    "            Pickle the list so that we don't have extract the data again\n",
    "        \"\"\"\n",
    "        with gzip.open(\"data/Amazon.pkl\", \"wb\") as az:\n",
    "            pickle.dump(reviews_df, az)\n",
    "            X, y = reviews_df.reviewText, reviews_df.overall\n",
    "        return train_test_split(X, y, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/final/Amazon.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c588c228f116>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mdf_comb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'data/S4/test.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0msplit_amazon_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-c588c228f116>\u001b[0m in \u001b[0;36msplit_amazon_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msplit_amazon_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/final/Amazon.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reviewText'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'overall'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'overall'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/gzip.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mgz_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"write\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/final/Amazon.pkl'"
     ]
    }
   ],
   "source": [
    "def split_amazon_data():\n",
    "    f = gzip.open(\"data/final/Amazon.pkl\", 'rb')\n",
    "    reviews = pickle.load(f)\n",
    "    df = reviews[['reviewText', 'overall']]\n",
    "    df.loc[:, 'overall'] = df.overall.apply(lambda x : 1 if x == 4 or x == 5 else 0)\n",
    "    df_pos = df.loc[df['overall'] == 1][:10000]\n",
    "    df_neg = df.loc[df['overall'] == 0][:10000]\n",
    "    df_comb = pd.concat([df_pos[:5000], df_neg[:5000]], axis=0)\n",
    "    df_comb2 = pd.concat([df_pos[5000:], df_neg[5000:]], axis=0)\n",
    "    df_comb = shuffle(df_comb, random_state=12)\n",
    "    df_comb2 = shuffle(df_comb2, random_state=12)\n",
    "\n",
    "    \n",
    "    df_comb.to_csv(r'data/S1/test.csv', index=False)\n",
    "    df_comb.to_csv(r'data/S2/test.csv', index=False)\n",
    "    df_comb.to_csv(r'data/S3/test.csv', index=False)\n",
    "    df_comb.to_csv(r'data/S4/test.csv', index=False)\n",
    "\n",
    "split_amazon_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_data():\n",
    "    imdb_file = 'data/final/imdb.csv'\n",
    "    imdb_df = pd.read_csv(imdb_file)\n",
    "    X, y = imdb_df.review, imdb_df.sentiment\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)    \n",
    "    df = pd.concat([X_train, y_train], axis=1)\n",
    "    df_dev = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    \"\"\"\n",
    "         Save as csv file\n",
    "    \"\"\"\n",
    "    df.to_csv(r'data/S1/train.csv', index=False)\n",
    "    df_dev.to_csv(r'data/S1/dev.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yelp_data():\n",
    "    \n",
    "    \"\"\" \n",
    "        Parameters:\n",
    "        test_size = float, optional (default=0.2)\n",
    "        \n",
    "        Returns:\n",
    "        Tuple containing training and test data for Yelp reviews\n",
    "        \n",
    "    \"\"\"\n",
    "    yelp_file = 'data/final/yelp.csv'\n",
    "    yelp_df = pd.read_csv(yelp_file)\n",
    "    yelp_df.loc[:, 'stars'] = yelp_df.stars.apply(lambda x : 1 if x == 4 or x == 5 else 0)\n",
    "    yelp_df = yelp_df[['text', 'stars']]\n",
    "    X, y = yelp_df.text, yelp_df.stars\n",
    "    \"\"\"\n",
    "        Split into train, test and validation set\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)    \n",
    "    df = pd.concat([X_train, y_train], axis=1)\n",
    "    df_dev = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    \"\"\"\n",
    "        Save as csv file\n",
    "    \"\"\"\n",
    "    df.to_csv(r'data/S2/train.csv', index=False)\n",
    "    df_dev.to_csv(r'data/S2/dev.csv', index=False)\n",
    "    print(df.shape)\n",
    "    print(df_dev.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_yelp_imbd():\n",
    "    \"\"\"\n",
    "        Read Yelp data and filter out only reviews and ratings\n",
    "    \"\"\"\n",
    "    \n",
    "    yelp_file = 'data/final/yelp.csv'\n",
    "    yelp_df = pd.read_csv(yelp_file)\n",
    "    yelp_df = yelp_df[['text', 'stars']]\n",
    "    yelp_df.loc[:, 'stars'] = yelp_df.stars.apply(lambda x : 1 if x == 4 or x == 5 else 0)\n",
    "    \n",
    "    \"\"\" \n",
    "        Read IMDB data and filter out reviews and ratings \n",
    "        Change columns name to match Yelp data since we will be combining the two datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    imdb_file = 'data/final/imdb.csv'\n",
    "    imdb_df = pd.read_csv(imdb_file)\n",
    "    imdb_df = imdb_df[['review', 'sentiment']]\n",
    "    imbd_df = imdb_df.rename(columns={'review':'text', 'sentiment':'stars'})\n",
    "    imbd_df.loc[:, 'stars'] = imbd_df.stars.apply(lambda x : 1 if x == \"positive\" else 0)\n",
    "\n",
    "    \"\"\"\n",
    "        Combine Yelp and IMDB reviews\n",
    "    \"\"\"\n",
    "    \n",
    "    yelp_imdb = pd.concat([yelp_df, imbd_df], axis=0)\n",
    "    X, y = yelp_imdb.text, yelp_imdb.stars\n",
    "    \n",
    "    \"\"\"\n",
    "        Split into train, test and validation set\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)    \n",
    "    \n",
    "    \"\"\"\n",
    "        Set up dataframes for train, test and validation set\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.concat([X_train, y_train], axis=1)\n",
    "    df_dev = pd.concat([X_test, y_test], axis=1)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "        Save train, test and validation set as csv file\n",
    "    \"\"\"\n",
    "    df.to_csv(r'data/S3/train.csv', index=False)\n",
    "    df_dev.to_csv(r'data/S3/dev.csv', index=False)\n",
    "    print(df.shape)\n",
    "    print(df_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 2)\n",
      "(12000, 2)\n"
     ]
    }
   ],
   "source": [
    "combine_yelp_imbd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_yelp_imdb_amz():\n",
    "    \"\"\"\n",
    "        Read Yelp data and filter out only reviews and ratings\n",
    "    \"\"\"\n",
    "\n",
    "    yelp_file = 'data/final/yelp.csv'\n",
    "    yelp_df = pd.read_csv(yelp_file)\n",
    "    yelp_df = yelp_df[['text', 'stars']]\n",
    "    yelp_df.loc[:, 'stars'] = yelp_df.stars.apply(lambda x : 1 if x == 4 or x == 5 else 0)\n",
    "\n",
    "    \"\"\" \n",
    "        Read IMDB data and filter out reviews and ratings \n",
    "        Change columns name to match Yelp data since we will be combining the two datasets\n",
    "    \"\"\"\n",
    "\n",
    "    imdb_file = 'data/final/imdb.csv'\n",
    "    imdb_df = pd.read_csv(imdb_file)\n",
    "    imdb_df = imdb_df[['review', 'sentiment']]\n",
    "    imbd_df = imdb_df.rename(columns={'review':'text', 'sentiment':'stars'})\n",
    "    imbd_df.loc[:, 'stars'] = imbd_df.stars.apply(lambda x : 1 if x == \"positive\" else 0)\n",
    "\n",
    "\n",
    "    f = gzip.open(\"data/final/Amazon.pkl\", 'rb')\n",
    "    reviews = pickle.load(f)\n",
    "    df = reviews[['reviewText', 'overall']]\n",
    "    df.loc[:, 'overall'] = df.overall.apply(lambda x : 1 if x == 4 or x == 5 else 0)\n",
    "    df_pos = df.loc[df['overall'] == 1][10000:]\n",
    "    df_neg = df.loc[df['overall'] == 0][10000:]\n",
    "    df_comb = pd.concat([df_pos[:10000], df_neg[:10000]], axis=0)\n",
    "    df_comb = shuffle(df_comb, random_state=12)\n",
    "    df_comb_az = df_comb.rename(columns={'reviewText':'text', 'overall':'stars'})\n",
    "\n",
    "    df_ayi = pd.concat([imbd_df, df_comb_az, yelp_df ], axis=0, sort=False)\n",
    "\n",
    "    X, y = df_ayi.text, df_ayi.stars\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)    \n",
    "    df = pd.concat([X_train, y_train], axis=1)\n",
    "    df_dev = pd.concat([X_test, y_test], axis=1)\n",
    "    df.to_csv(r'data/S4/train.csv', index=False)\n",
    "    df_dev.to_csv(r'data/S4/dev.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_yelp_imdb_amz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
