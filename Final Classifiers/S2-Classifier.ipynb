{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"S2-Classifier.ipynb","provenance":[{"file_id":"10AByvDXnlMVOD1SPO4Wa8sSS_nR_iol7","timestamp":1574368176261}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"9Ds_At2DplQD","colab_type":"text"},"source":["# **Classifier #2**\n","Train: Yelp  \n","Test: Amazon"]},{"cell_type":"markdown","metadata":{"id":"pcisnsYp2SVZ","colab_type":"text"},"source":["# **Create Corpus**"]},{"cell_type":"code","metadata":{"id":"qK4W1plE16fb","colab_type":"code","colab":{}},"source":["pip install flair # must install package if first execution"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3tMwpXsAzQYz","colab_type":"code","outputId":"4a97690b-f13f-47fc-e8ef-048df0d7f166","executionInfo":{"status":"ok","timestamp":1574389752032,"user_tz":360,"elapsed":3376,"user":{"displayName":"Payton Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCPs1T6TM9X-ZFb3qy-FRZg_09KzlL2RwOi0ldjhA=s64","userId":"07447174154653828546"}},"colab":{"base_uri":"https://localhost:8080/","height":148}},"source":["from flair.data import Corpus\n","from flair.datasets import CSVClassificationCorpus\n","\n","# set folder containing the corpus data\n","data_folder = 'data/S1'\n","\n","# define which columns contain the review text and the sentiment label\n","column_name_map = {0: \"text\", 1: \"label_topic\"}\n","\n","# create corpus \n","corpus: Corpus = CSVClassificationCorpus(data_folder, column_name_map, skip_header=True, delimiter=',',)"],"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["2019-11-22 02:29:10,792 Reading data from data\n","2019-11-22 02:29:10,793 Train: data/train.csv\n","2019-11-22 02:29:10,794 Dev: data/dev.csv\n","2019-11-22 02:29:10,795 Test: data/test.csv\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GgksOzU12Ygp","colab_type":"text"},"source":["# **Train Model**"]},{"cell_type":"code","metadata":{"id":"WRzLWM-W10W0","colab_type":"code","colab":{}},"source":["from flair.data import Corpus\n","from flair.datasets import IMDB\n","from flair.embeddings import DocumentRNNEmbeddings, XLNetEmbeddings\n","from flair.embeddings import StackedEmbeddings\n","from flair.models import TextClassifier\n","from flair.trainers import ModelTrainer\n","\n","# print corpus distribution (train, dev, test)\n","print(corpus)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_LK9-acA2otV","colab_type":"code","outputId":"eeb504d5-9194-47bb-e914-506618808551","executionInfo":{"status":"ok","timestamp":1574389797314,"user_tz":360,"elapsed":36104,"user":{"displayName":"Payton Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCPs1T6TM9X-ZFb3qy-FRZg_09KzlL2RwOi0ldjhA=s64","userId":"07447174154653828546"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# build label dictionary\n","label_dict = corpus.make_label_dictionary()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["2019-11-22 02:29:20,781 Computing label dictionary. Progress:\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 8000/8000 [00:35<00:00, 225.17it/s]"],"name":"stderr"},{"output_type":"stream","text":["2019-11-22 02:29:56,465 [b'1', b'0']\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"GmUUiDnLjmFK","colab_type":"code","colab":{}},"source":["# define XLNET word embeddings\n","word_embeddings = [XLNetEmbeddings('xlnet-base-cased')]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sfs18Bz44Dtv","colab_type":"code","colab":{}},"source":["# define a document embedding using the XLNET word embeddings defined above\n","document_embeddings: DocumentRNNEmbeddings = DocumentRNNEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256,)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UT10gpOW4JG_","colab_type":"code","colab":{}},"source":["# build a text classifier\n","classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4IY7Y-WJ4NRe","colab_type":"code","colab":{}},"source":["# build the text classifier trainer object\n","trainer = ModelTrainer(classifier, corpus)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AcHLw7H64PyO","colab_type":"code","outputId":"cf0c280a-09a8-40a0-bd3b-235e96014020","executionInfo":{"status":"ok","timestamp":1574396945459,"user_tz":360,"elapsed":495292,"user":{"displayName":"Payton Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCPs1T6TM9X-ZFb3qy-FRZg_09KzlL2RwOi0ldjhA=s64","userId":"07447174154653828546"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# 7. start the training\n","  # 'resources/taggers/sentiment-yelp' is the location where you want to save your model files.\n","trainer.train('resources/taggers/sentiment-yelp',\n","              learning_rate=0.1,\n","              mini_batch_size=32,\n","              anneal_factor=0.5,\n","              patience=5,\n","              max_epochs=5)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["2019-11-22 02:30:18,016 ----------------------------------------------------------------------------------------------------\n","2019-11-22 02:30:18,022 Model: \"TextClassifier(\n","  (document_embeddings): DocumentRNNEmbeddings(\n","    (embeddings): StackedEmbeddings(\n","      (list_embedding_0): XLNetEmbeddings(\n","        model=0-xlnet-base-cased\n","        (model): XLNetModel(\n","          (word_embedding): Embedding(32000, 768)\n","          (layer): ModuleList(\n","            (0): XLNetLayer(\n","              (rel_attn): XLNetRelativeAttention(\n","                (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (ff): XLNetFeedForward(\n","                (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","                (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): XLNetLayer(\n","              (rel_attn): XLNetRelativeAttention(\n","                (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (ff): XLNetFeedForward(\n","                (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","                (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (2): XLNetLayer(\n","              (rel_attn): XLNetRelativeAttention(\n","                (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (ff): XLNetFeedForward(\n","                (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","                (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (3): XLNetLayer(\n","              (rel_attn): XLNetRelativeAttention(\n","                (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (ff): XLNetFeedForward(\n","                (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","                (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (4): XLNetLayer(\n","              (rel_attn): XLNetRelativeAttention(\n","                (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (ff): XLNetFeedForward(\n","                (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","                (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (5): XLNetLayer(\n","              (rel_attn): XLNetRelativeAttention(\n","                (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (ff): XLNetFeedForward(\n","                (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","                (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (6): XLNetLayer(\n","              (rel_attn): XLNetRelativeAttention(\n","                (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (ff): XLNetFeedForward(\n","                (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","                (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (7): XLNetLayer(\n","              (rel_attn): XLNetRelativeAttention(\n","                (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (ff): XLNetFeedForward(\n","                (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","                (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (8): XLNetLayer(\n","              (rel_attn): XLNetRelativeAttention(\n","                (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (ff): XLNetFeedForward(\n","                (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","                (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (9): XLNetLayer(\n","              (rel_attn): XLNetRelativeAttention(\n","                (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (ff): XLNetFeedForward(\n","                (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","                (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (10): XLNetLayer(\n","              (rel_attn): XLNetRelativeAttention(\n","                (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (ff): XLNetFeedForward(\n","                (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","                (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (11): XLNetLayer(\n","              (rel_attn): XLNetRelativeAttention(\n","                (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (ff): XLNetFeedForward(\n","                (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","                (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (word_reprojection_map): Linear(in_features=1536, out_features=256, bias=True)\n","    (rnn): GRU(256, 512, batch_first=True)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n","  (decoder): Linear(in_features=512, out_features=2, bias=True)\n","  (loss_function): CrossEntropyLoss()\n",")\"\n","2019-11-22 02:30:18,024 ----------------------------------------------------------------------------------------------------\n","2019-11-22 02:30:18,025 Corpus: \"Corpus: 8000 train + 2000 dev + 10000 test sentences\"\n","2019-11-22 02:30:18,027 ----------------------------------------------------------------------------------------------------\n","2019-11-22 02:30:18,029 Parameters:\n","2019-11-22 02:30:18,031  - learning_rate: \"0.1\"\n","2019-11-22 02:30:18,037  - mini_batch_size: \"32\"\n","2019-11-22 02:30:18,039  - patience: \"5\"\n","2019-11-22 02:30:18,041  - anneal_factor: \"0.5\"\n","2019-11-22 02:30:18,045  - max_epochs: \"5\"\n","2019-11-22 02:30:18,047  - shuffle: \"True\"\n","2019-11-22 02:30:18,049  - train_with_dev: \"False\"\n","2019-11-22 02:30:18,051  - batch_growth_annealing: \"False\"\n","2019-11-22 02:30:18,052 ----------------------------------------------------------------------------------------------------\n","2019-11-22 02:30:18,054 Model training base path: \"resources/taggers/sentiment-yelp\"\n","2019-11-22 02:30:18,056 ----------------------------------------------------------------------------------------------------\n","2019-11-22 02:30:18,058 Device: cuda:0\n","2019-11-22 02:30:18,060 ----------------------------------------------------------------------------------------------------\n","2019-11-22 02:30:18,062 Embeddings storage mode: cpu\n","2019-11-22 02:30:18,168 ----------------------------------------------------------------------------------------------------\n","2019-11-22 02:30:24,653 epoch 1 - iter 0/250 - loss 0.80069762 - samples/sec: 229.60\n","2019-11-22 02:31:59,859 epoch 1 - iter 25/250 - loss 0.75826647 - samples/sec: 8.65\n","2019-11-22 02:33:33,085 epoch 1 - iter 50/250 - loss 0.70300402 - samples/sec: 8.73\n","2019-11-22 02:35:10,266 epoch 1 - iter 75/250 - loss 0.68576230 - samples/sec: 8.42\n","2019-11-22 02:36:47,510 epoch 1 - iter 100/250 - loss 0.66484271 - samples/sec: 8.42\n","2019-11-22 02:38:19,943 epoch 1 - iter 125/250 - loss 0.65474448 - samples/sec: 8.86\n","2019-11-22 02:39:56,919 epoch 1 - iter 150/250 - loss 0.65102534 - samples/sec: 8.43\n","2019-11-22 02:41:35,293 epoch 1 - iter 175/250 - loss 0.64438280 - samples/sec: 8.35\n","2019-11-22 02:43:08,186 epoch 1 - iter 200/250 - loss 0.63781206 - samples/sec: 8.82\n","2019-11-22 02:44:44,681 epoch 1 - iter 225/250 - loss 0.63166458 - samples/sec: 8.50\n","2019-11-22 02:46:13,812 ----------------------------------------------------------------------------------------------------\n","2019-11-22 02:46:13,814 EPOCH 1 done: loss 0.6282 - lr 0.1000\n","2019-11-22 02:50:02,824 DEV : loss 0.5552495121955872 - score 0.7185\n","2019-11-22 02:50:13,970 BAD EPOCHS (no improvement): 0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type XLNetEmbeddings. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type XLNetModel. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type XLNetLayer. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type XLNetRelativeAttention. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LayerNorm. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type XLNetFeedForward. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"},{"output_type":"stream","text":["2019-11-22 02:50:15,709 ----------------------------------------------------------------------------------------------------\n","2019-11-22 02:50:22,220 epoch 2 - iter 0/250 - loss 0.51095915 - samples/sec: 175.25\n","2019-11-22 02:51:55,317 epoch 2 - iter 25/250 - loss 0.59434005 - samples/sec: 8.82\n","2019-11-22 02:53:31,992 epoch 2 - iter 50/250 - loss 0.58716238 - samples/sec: 8.48\n","2019-11-22 02:55:06,705 epoch 2 - iter 75/250 - loss 0.59185657 - samples/sec: 8.63\n","2019-11-22 02:56:44,651 epoch 2 - iter 100/250 - loss 0.57491104 - samples/sec: 8.36\n","2019-11-22 02:58:17,451 epoch 2 - iter 125/250 - loss 0.57212733 - samples/sec: 8.82\n","2019-11-22 02:59:49,118 epoch 2 - iter 150/250 - loss 0.56161584 - samples/sec: 8.99\n","2019-11-22 03:01:24,078 epoch 2 - iter 175/250 - loss 0.56614225 - samples/sec: 8.62\n","2019-11-22 03:02:57,164 epoch 2 - iter 200/250 - loss 0.56147406 - samples/sec: 8.80\n","2019-11-22 03:04:35,712 epoch 2 - iter 225/250 - loss 0.56306811 - samples/sec: 8.34\n","2019-11-22 03:06:02,678 ----------------------------------------------------------------------------------------------------\n","2019-11-22 03:06:02,680 EPOCH 2 done: loss 0.5602 - lr 0.1000\n","2019-11-22 03:09:47,878 DEV : loss 0.6022749543190002 - score 0.7095\n","2019-11-22 03:09:58,731 BAD EPOCHS (no improvement): 1\n","2019-11-22 03:09:58,733 ----------------------------------------------------------------------------------------------------\n","2019-11-22 03:10:06,757 epoch 3 - iter 0/250 - loss 0.69587362 - samples/sec: 163.70\n","2019-11-22 03:11:42,036 epoch 3 - iter 25/250 - loss 0.59377508 - samples/sec: 8.62\n","2019-11-22 03:13:18,086 epoch 3 - iter 50/250 - loss 0.58819832 - samples/sec: 8.58\n","2019-11-22 03:14:54,709 epoch 3 - iter 75/250 - loss 0.56222603 - samples/sec: 8.48\n","2019-11-22 03:16:27,627 epoch 3 - iter 100/250 - loss 0.54976330 - samples/sec: 8.82\n","2019-11-22 03:18:03,344 epoch 3 - iter 125/250 - loss 0.54988041 - samples/sec: 8.58\n","2019-11-22 03:19:34,473 epoch 3 - iter 150/250 - loss 0.54553985 - samples/sec: 9.07\n","2019-11-22 03:21:10,330 epoch 3 - iter 175/250 - loss 0.54463323 - samples/sec: 8.57\n","2019-11-22 03:22:45,000 epoch 3 - iter 200/250 - loss 0.53969144 - samples/sec: 8.71\n","2019-11-22 03:24:23,104 epoch 3 - iter 225/250 - loss 0.53855634 - samples/sec: 8.37\n","2019-11-22 03:25:57,201 ----------------------------------------------------------------------------------------------------\n","2019-11-22 03:25:57,203 EPOCH 3 done: loss 0.5361 - lr 0.1000\n","2019-11-22 03:29:51,633 DEV : loss 0.5346253514289856 - score 0.7305\n","2019-11-22 03:30:03,625 BAD EPOCHS (no improvement): 0\n","2019-11-22 03:30:05,455 ----------------------------------------------------------------------------------------------------\n","2019-11-22 03:30:11,470 epoch 4 - iter 0/250 - loss 0.61179304 - samples/sec: 200.82\n","2019-11-22 03:31:48,752 epoch 4 - iter 25/250 - loss 0.52638720 - samples/sec: 8.41\n","2019-11-22 03:33:23,704 epoch 4 - iter 50/250 - loss 0.52315728 - samples/sec: 8.63\n","2019-11-22 03:35:01,711 epoch 4 - iter 75/250 - loss 0.53062298 - samples/sec: 8.35\n","2019-11-22 03:36:41,700 epoch 4 - iter 100/250 - loss 0.52424126 - samples/sec: 8.20\n","2019-11-22 03:38:14,320 epoch 4 - iter 125/250 - loss 0.52219118 - samples/sec: 8.82\n","2019-11-22 03:39:48,469 epoch 4 - iter 150/250 - loss 0.51958179 - samples/sec: 8.74\n","2019-11-22 03:41:31,886 epoch 4 - iter 175/250 - loss 0.51480556 - samples/sec: 7.93\n","2019-11-22 03:43:05,220 epoch 4 - iter 200/250 - loss 0.51096358 - samples/sec: 8.79\n","2019-11-22 03:44:38,659 epoch 4 - iter 225/250 - loss 0.51121024 - samples/sec: 8.72\n","2019-11-22 03:46:10,922 ----------------------------------------------------------------------------------------------------\n","2019-11-22 03:46:10,924 EPOCH 4 done: loss 0.5125 - lr 0.1000\n","2019-11-22 03:50:01,924 DEV : loss 0.47645261883735657 - score 0.782\n","2019-11-22 03:50:13,321 BAD EPOCHS (no improvement): 0\n","2019-11-22 03:50:14,907 ----------------------------------------------------------------------------------------------------\n","2019-11-22 03:50:21,653 epoch 5 - iter 0/250 - loss 0.54990858 - samples/sec: 226.91\n","2019-11-22 03:51:55,385 epoch 5 - iter 25/250 - loss 0.47871543 - samples/sec: 8.78\n","2019-11-22 03:53:29,683 epoch 5 - iter 50/250 - loss 0.49346472 - samples/sec: 8.75\n","2019-11-22 03:55:03,952 epoch 5 - iter 75/250 - loss 0.49476467 - samples/sec: 8.78\n","2019-11-22 03:56:41,657 epoch 5 - iter 100/250 - loss 0.48213658 - samples/sec: 8.42\n","2019-11-22 03:58:15,844 epoch 5 - iter 125/250 - loss 0.48376182 - samples/sec: 8.71\n","2019-11-22 03:59:53,155 epoch 5 - iter 150/250 - loss 0.48588707 - samples/sec: 8.46\n","2019-11-22 04:01:29,372 epoch 5 - iter 175/250 - loss 0.48654265 - samples/sec: 8.52\n","2019-11-22 04:03:03,426 epoch 5 - iter 200/250 - loss 0.48603112 - samples/sec: 8.75\n","2019-11-22 04:04:43,639 epoch 5 - iter 225/250 - loss 0.48710421 - samples/sec: 8.23\n","2019-11-22 04:06:16,549 ----------------------------------------------------------------------------------------------------\n","2019-11-22 04:06:16,552 EPOCH 5 done: loss 0.4897 - lr 0.1000\n","2019-11-22 04:10:08,021 DEV : loss 0.4671704173088074 - score 0.8035\n","2019-11-22 04:10:19,283 BAD EPOCHS (no improvement): 0\n","2019-11-22 04:10:23,151 ----------------------------------------------------------------------------------------------------\n","2019-11-22 04:10:23,152 Testing using best model ...\n","2019-11-22 04:10:23,156 loading file resources/taggers/sentiment-yelp/best-model.pt\n","2019-11-22 04:29:04,393 0.7114\t0.7114\t0.7114\n","2019-11-22 04:29:04,395 \n","MICRO_AVG: acc 0.5521 - f1-score 0.7114\n","MACRO_AVG: acc 0.552 - f1-score 0.7113499999999999\n","0          tp: 3492 - fp: 1378 - fn: 1508 - tn: 3622 - precision: 0.7170 - recall: 0.6984 - accuracy: 0.5475 - f1-score: 0.7076\n","1          tp: 3622 - fp: 1508 - fn: 1378 - tn: 3492 - precision: 0.7060 - recall: 0.7244 - accuracy: 0.5565 - f1-score: 0.7151\n","2019-11-22 04:29:04,397 ----------------------------------------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'test_score': 0.7114,\n"," 'dev_score_history': [0.7185, 0.7095, 0.7305, 0.782, 0.8035],\n"," 'train_loss_history': [0.6281571127176285,\n","  0.5601590470075607,\n","  0.5360901998281479,\n","  0.5124937475323678,\n","  0.4897082913517952],\n"," 'dev_loss_history': [tensor(0.5552, device='cuda:0'),\n","  tensor(0.6023, device='cuda:0'),\n","  tensor(0.5346, device='cuda:0'),\n","  tensor(0.4765, device='cuda:0'),\n","  tensor(0.4672, device='cuda:0')]}"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"apilnWJV8iN9","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}