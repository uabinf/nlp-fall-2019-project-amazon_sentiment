{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "\n",
    "\"\"\"\n",
    "    Loading the BERT base model onto the gpu before any of the preprocessing steps to avoid out of memory error on\n",
    "    cheaha\n",
    "\"\"\"\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle as pk\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Store the device. We transfer our data on this ydevice during training\n",
    "\"\"\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Open the pickled Amazon reviews file\n",
    "\"\"\"\n",
    "f =gzip.open('data/final/Amazon.pkl', 'rb')\n",
    "reviews = pk.load(f)\n",
    "reviews = reviews.dropna()\n",
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Read Yelp Data\n",
    "\"\"\"\n",
    "yelp = pd.read_csv('data/final/yelp.csv')\n",
    "yelp = yelp[['text', 'stars']]\n",
    "yelp.loc[:, 'stars'] = yelp.stars.apply(lambda x : 1 if x == 4 or x == 5 else 0)\n",
    "yelp = yelp.dropna()\n",
    "yelp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Read IMDB data\n",
    "\"\"\"\n",
    "imdb = pd.read_csv('data/final/imdb.csv')\n",
    "imdb = imdb[['review', 'sentiment']]\n",
    "imdb = imdb.rename(columns={'review':'text', 'sentiment':'stars'})\n",
    "imdb.loc[:, 'stars'] = imdb.stars.apply(lambda x : 1 if x == \"positive\" else 0)\n",
    "imdb = imdb.dropna()\n",
    "imdb = imdb.replace(r'\\<br /><br />',' ', regex=True) #Remove line breaks\n",
    "imdb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Separate our reviews and ratings. Using only a 100 reviews for initial reviews\n",
    "    Convert ratings into a binary class. 1-3 stars as negative (0) and 4-5 as positive (1)\n",
    "\"\"\"\n",
    "\n",
    "text = reviews.reviewText.values[:100000] # Using 100k for faster training, can be increased\n",
    "labels = reviews.overall.apply(lambda x : 1 if x == 4 or x == 5 else 0 ).values[:100000]\n",
    "text = [\"[CLS] \" + sent + \" [SEP]\" for sent in text]\n",
    "\n",
    "\"\"\"\n",
    "    Format the review text into the format required for BERT\n",
    "    by adding [CLS] and [SEP] tokens \n",
    "\"\"\"\n",
    "yelp_text = yelp.text.values\n",
    "yelp_labels = yelp.stars.values\n",
    "yelp_text =  [\"[CLS] \" + sent + \" [SEP]\" for sent in yelp_text]\n",
    "\n",
    "imdb_text = imdb.text.values\n",
    "imdb_labels = imdb.stars.values\n",
    "imdb_text =  [\"[CLS] \" + sent + \" [SEP]\" for sent in imdb_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Lower cased bert base tokenizer\n",
    "\"\"\"\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\"\"\"\n",
    "    Use BERT Tokenizer to tokenize our review text \n",
    "\"\"\"\n",
    "tokenized_text_az = [tokenizer.tokenize(sent) for sent in text]\n",
    "tokenized_text_yelp = [tokenizer.tokenize(sent) for sent in yelp_text]\n",
    "tokenized_text_imdb = [tokenizer.tokenize(sent) for sent in imdb_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon reviews length after trim: 93310\n",
      "Amazon rating length after trim: 93310\n",
      "Yelp reviews length after trim: 9661\n",
      "Yelp rating length after trim: 9661\n",
      "IMDB reviews length after trim: 43569\n",
      "IMDB rating length after trim: 43569\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Because BERT expects a sequence length of 512 or less, we are extracting reviews that are less than 512 in length.\n",
    "    Try reducing the sequence length to 128 if there are memory issues on cheaha\n",
    "\"\"\"\n",
    "def trim_sentences(tokenized_list, label):\n",
    "    long_seq = []\n",
    "    ratings = []\n",
    "    for idx, tok in enumerate(tokenized_list):\n",
    "        if len(tok) < 512:\n",
    "            long_seq.append(tok)\n",
    "            ratings.append(label[idx])\n",
    "    return (long_seq, ratings)\n",
    "\n",
    "long_seq, ratings = trim_sentences(tokenized_text_az, labels)\n",
    "long_seq_yelp, yelp_ratings = trim_sentences(tokenized_text_yelp, yelp_labels)\n",
    "long_seq_imdb, imdb_ratings = trim_sentences(tokenized_text_imdb, imdb_labels)\n",
    "\n",
    "\n",
    "print(f\"Amazon reviews length after trim: {len(long_seq)}\")\n",
    "print(f\"Amazon rating length after trim: {len(ratings)}\")\n",
    "\n",
    "print(f\"Yelp reviews length after trim: {len(long_seq_yelp)}\")\n",
    "print(f\"Yelp rating length after trim: {len(yelp_ratings)}\")\n",
    "\n",
    "print(f\"IMDB reviews length after trim: {len(long_seq_imdb)}\")\n",
    "print(f\"IMDB rating length after trim: {len(imdb_ratings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    BERT expects a input id for each token in our reviews. Using a function from the BERTTokenizer to \n",
    "    convert tokens into ids\n",
    "\"\"\"\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(token) for token in long_seq]\n",
    "yelp_input_ids = [tokenizer.convert_tokens_to_ids(token) for token in long_seq_yelp]\n",
    "imdb_input_ids = [tokenizer.convert_tokens_to_ids(token) for token in long_seq_imdb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Convert all input_ids into a uniform length of 128. Add padding to tokens less than 128 in length.\n",
    "    \n",
    "    Again, the max length here can be higher but using 128 to see if it fixes memory uses around Cheaha\n",
    "    \n",
    "\"\"\"\n",
    "input_ids = pad_sequences(input_ids, maxlen=128, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "yelp_input_ids = pad_sequences(yelp_input_ids, maxlen=128, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "imdb_input_ids = pad_sequences(imdb_input_ids, maxlen=128, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Setting up attention masks to input into the model\n",
    "\"\"\"\n",
    "def generate_attention_masks(input_ids):\n",
    "    attention_masks = []\n",
    "    for seq in input_ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "    return attention_masks\n",
    "attention_masks = generate_attention_masks(input_ids)\n",
    "attention_masks_yelp = generate_attention_masks(yelp_input_ids)\n",
    "attention_masks_imdb = generate_attention_masks(imdb_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Set train and validation data\n",
    "\"\"\"\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, ratings, \n",
    "                                                            random_state=18, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=18, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Convert our data into tensors\n",
    "\"\"\"\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "yelp_validation_inputs = torch.tensor(yelp_input_ids)\n",
    "yelp_validation_labels = torch.tensor(yelp_ratings)\n",
    "yelp_validation_masks = torch.tensor(attention_masks_yelp)\n",
    "\n",
    "imdb_validation_inputs = torch.tensor(imdb_input_ids)\n",
    "imdb_validation_labels = torch.tensor(imdb_ratings)\n",
    "imdb_validation_masks = torch.tensor(attention_masks_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Recommed batch sizes are 16 or 32 but use a small batch size if there are memory issues on Cheaha\n",
    "    \n",
    "\"\"\"\n",
    "batch_size = 32\n",
    "\n",
    "\"\"\"\n",
    "    Get an iterable dataset for Amazon\n",
    "\"\"\"\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Get an iterable dataset for Yelp\n",
    "\"\"\"\n",
    "yelp_validation_data = TensorDataset(yelp_validation_inputs, yelp_validation_masks, yelp_validation_labels)\n",
    "yelp_validation_sampler = SequentialSampler(yelp_validation_data)\n",
    "yelp_validation_dataloader = DataLoader(yelp_validation_data, sampler=yelp_validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Get an iterable dataset for IMDB\n",
    "\"\"\"\n",
    "imdb_validation_data = TensorDataset(imdb_validation_inputs, imdb_validation_masks, imdb_validation_labels)\n",
    "imdb_validation_sampler = SequentialSampler(imdb_validation_data)\n",
    "imdb_validation_dataloader = DataLoader(imdb_validation_data, sampler=imdb_validation_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Get the parameter names from model and setup the parameters to be passed\n",
    "    to the Adam optimizer in the next step\n",
    "\"\"\"\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Initialize Adam optimizer\n",
    "\"\"\"\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=2e-5,\n",
    "                     warmup=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Predict accuracy of our model\n",
    "\"\"\"\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.7195597290992737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  25%|██▌       | 1/4 [21:02<1:03:08, 1262.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Accuracy: 0.8421052631578947\n",
      "\n",
      "Train loss: 0.09748785942792892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 2/4 [41:47<41:55, 1257.50s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Accuracy: 0.8947368421052632\n",
      "\n",
      "Train loss: 0.01057206466794014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  75%|███████▌  | 3/4 [1:02:30<20:53, 1253.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Accuracy: 0.8421052631578947\n",
      "\n",
      "Train loss: 0.0010580149246379733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 4/4 [1:23:14<00:00, 1250.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Accuracy: 0.8947368421052632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "   Store our training losss and set the number of epochs. \n",
    "   Fine-tuning requires 2 to 4 passes over the entire dataset\n",
    "\"\"\"\n",
    "train_loss_set = []\n",
    "epochs = 4\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    \"\"\"\n",
    "       Start Training\n",
    "       \"\"\"\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    \"\"\"\n",
    "       Loop over the data set. Transform the tensors to the gpu. \n",
    "       Provide the training data to the model.\n",
    "       Performa forward and backward pass.\n",
    "       Store the loss and update parameters for the optimizers\n",
    "       \"\"\"\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad() # clear gradient after every step\n",
    "        loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        train_loss_set.append(loss.item())    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "    tr_loss += loss.item()\n",
    "    nb_tr_examples += b_input_ids.size(0)\n",
    "    nb_tr_steps += 1\n",
    "    print(\"\\nTrain loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    \"\"\"\n",
    "        Evaluate our trained model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "  \n",
    "    \"\"\"\n",
    "        Make predictions on our validation dataset \n",
    "    \"\"\"\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    \n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "    print(\"\\nValidation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "model.eval()\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "preds , true_labels = [], []\n",
    "\"\"\"\n",
    "   Make predictions on our validation dataset \n",
    "\"\"\"\n",
    "for batch in yelp_validation_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    with torch.no_grad():\n",
    "        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "    preds.append(logits)\n",
    "    true_labels.append(label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_predictions = [item for sublist in preds for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_test =f1_score(flat_predictions, flat_true_labels)\n",
    "recall_test = recall_score(flat_predictions, flat_true_labels)\n",
    "precision_test = precision_score(flat_predictions, flat_true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy for Yelp: 0.8396822904772779\n",
      "F1 Score for Yelp: 0.8929435344529684\n",
      "Recall Score  for Yelp: 0.8292682926829268\n",
      "Precision Score for Yelp: 0.9672106602784848\n"
     ]
    }
   ],
   "source": [
    "print(f\"Validation Accuracy for Yelp: {format(eval_accuracy/nb_eval_steps)}\")\n",
    "print(f\"F1 Score for Yelp: {format(f1_test)}\")\n",
    "print(f\"Recall Score  for Yelp: {format(recall_test)}\")\n",
    "print(f\"Precision Score for Yelp: {format(precision_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_az = []\n",
    "labels_az = []\n",
    "eval_loss, eval_accuracy= 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "for batch in validation_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    with torch.no_grad():\n",
    "        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "    preds_az.append(logits)\n",
    "    labels_az.append(label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_predictions = [item for sublist in preds_az for item in sublist]\n",
    "flat_predictions_az = np.argmax(flat_predictions, axis=1).flatten()\n",
    "flat_true_labels_az = [item for sublist in labels_az for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_test =f1_score(flat_predictions_az, flat_true_labels_az)\n",
    "recall_test = recall_score(flat_predictions_az, flat_true_labels_az)\n",
    "precision_test = precision_score(flat_predictions_az, flat_true_labels_az)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy for Amazon: 0.9019297494592646\n",
      "F1 Score for Amazon: 0.9342814048696402\n",
      "Recall Score  for Amazon: 0.9347513653348664\n",
      "Precision Score for Amazon: 0.9338119167264896\n"
     ]
    }
   ],
   "source": [
    "print(f\"Validation Accuracy for Amazon: {format(eval_accuracy/nb_eval_steps)}\")\n",
    "print(f\"F1 Score for Amazon: {format(f1_test)}\")\n",
    "print(f\"Recall Score  for Amazon: {format(recall_test)}\")\n",
    "print(f\"Precision Score for Amazon: {format(precision_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_im = []\n",
    "labels_im = []\n",
    "eval_loss, eval_accuracy= 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "for batch in imdb_validation_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    with torch.no_grad():\n",
    "        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "    preds_im.append(logits)\n",
    "    labels_im.append(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_predictions = [item for sublist in preds_im for item in sublist]\n",
    "flat_predictions_im = np.argmax(flat_predictions, axis=1).flatten()\n",
    "flat_true_labels_im = [item for sublist in labels_im for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_test =f1_score(flat_predictions_im, flat_true_labels_im)\n",
    "recall_test = recall_score(flat_predictions_im, flat_true_labels_im)\n",
    "precision_test = precision_score(flat_predictions_im, flat_true_labels_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy for IMDB: 0.815786419193228\n",
      "F1 Score for IMDB: 0.8296585531481443\n",
      "Recall Score  for IMDB: 0.7672501766229688\n",
      "Precision Score for IMDB: 0.9031185031185032\n"
     ]
    }
   ],
   "source": [
    "print(f\"Validation Accuracy for IMDB: {format(eval_accuracy/nb_eval_steps)}\")\n",
    "print(f\"F1 Score for IMDB: {format(f1_test)}\")\n",
    "print(f\"Recall Score  for IMDB: {format(recall_test)}\")\n",
    "print(f\"Precision Score for IMDB: {format(precision_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DeepNLP]",
   "language": "python",
   "name": "conda-env-DeepNLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
